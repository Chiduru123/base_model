#!/usr/bin/env python
# encoding: utf-8
'''
# Author:        Guo Qi
# File:          RNN.py
# Date:          2020/7/16
# Description:   ğŸ£RNN
'''

import numpy as np
from utils import rnn_utils
from utils import cllm_utils
import time


def cell_forward(xt, a_prev, params):
    '''
    ç¥ç»å…ƒä¸Šçš„å•æ­¥å‰å‘ä¼ æ’­, tanh + softmax
    :param xt:       shape = n_a, m
    :param a_prev:   shape = n_a, m
    :param params:   dict of params { Waa, Wax, Way, ba, by }
    :return:
        a_next - shape=n_a, m
        y -  shape = n_y, m
        cache - åå‘å·¥å…· (a_next, a_prev, xt, params)
    '''
    Wax, Waa, Way, ba, by = params['Wax'], params['Waa'], params['Wya'], params['ba'], params['by']

    a_next = np.tanh(np.dot(Waa, a_prev) + np.dot(Wax, xt) + ba)
    y = rnn_utils.softmax(np.dot(Way, a_next) + by)

    cache = (a_next, a_prev, xt, params)
    return a_next, y, cache


def cell_backward(da_next, cache):
    '''
    å•ç¥ç»å…ƒä¸Šçš„å•å±‚åå‘ä¼ æ’­
    '''
    a_next, a_prev, xt, param = cache
    Waa, Wax, Wya, ba, by = param['Waa'], param['Wax'], param['Wya'], param['ba'], param['by']

    dtanh = (1 - np.square(a_next)) * da_next

    dWaa = np.dot(dtanh, a_prev.T)
    dWax = np.dot(dtanh, xt.T)
    dba = np.sum(dtanh, axis=1, keepdims=True)
    dxt = np.dot(Wax.T, dtanh)
    da_prev = np.dot(Waa.T, dtanh)

    gradients = {"dxt": dxt, "da_prev": da_prev, "dWax": dWax, "dWaa": dWaa, "dba": dba}
    return gradients


def rnn_forward(x, a0, params):
    '''
    RNNç½‘ç»œçš„å‰å‘ä¼ æ’­
    '''
    caches = []
    n_x, m, T_x = x.shape
    n_y, n_a = params['Wya'].shape

    a = np.zeros((n_a, m, T_x))
    y_pred = np.zeros((n_y, m, T_x))
    a_next = a0

    for t in range(T_x):
        a_next, yt, cache = cell_forward(x[:, :, t], a_prev=a_next, params=params)
        a[:, :, t] = a_next
        y_pred[:, :, t] = yt
        caches.append(cache)

    caches = (caches, x)
    return a, y_pred, caches


def rnn_backward(da, caches):
    '''
    rnnåå‘ä¼ æ’­
    :param da:
    :param cache:
    :return:
    '''
    caches, x = caches
    a1, a0, xt, param = caches[0]

    n_a, m, T_x = da.shape
    n_x, m = xt.shape

    dx = np.zeros([n_x, m, T_x])
    dWax = np.zeros([n_a, n_x])
    dWaa = np.zeros([n_a, n_a])
    dba = np.zeros([n_a, 1])
    da_prev = np.zeros([n_a, m])

    for t in reversed(range(T_x)):
        grads = cell_backward(da_next=da[:, :, t] + da_prev, cache=caches[t])
        dxt, da_prevt, dWaxt, dWaat, dbat = grads["dxt"], grads["da_prev"], grads["dWax"], grads["dWaa"], grads["dba"]
        dx[:, :, t] = dxt

        dWax += dWaxt
        dWaa += dWaat
        dba += dbat

    da0 = da_prevt

    grads = {
        'dWaa': dWaa,
        'dWax': dWax,
        'dba': dba,
        'dx':  dx,
        'da0': da0
    }
    return grads


def clip(grads, maxValue):
    '''
    æ¢¯åº¦ä¿®å‰ª, é¿å…æ¢¯åº¦çˆ†ç‚¸
    '''
    dWaa, dWax, dWya, db, dby = grads['dWaa'], grads['dWax'], grads['dWya'], grads['db'], grads['dby']
    for grad in [dWaa, dWax, dWya, db, dby]:
        np.clip(grad, -maxValue, maxValue, out=grad)
    grads['dWaa'], grads['dWax'], grads['dWya'], grads['db'], grads['dby'] = dWaa, dWax, dWya, db, dby
    return grads


def sample(parameters, char_to_is, seed):
    """
    æ ¹æ®RNNè¾“å‡ºçš„æ¦‚ç‡åˆ†å¸ƒåºåˆ—å¯¹å­—ç¬¦åºåˆ—è¿›è¡Œé‡‡æ ·
    å‚æ•°ï¼š
        parameters -- åŒ…å«äº†Waa, Wax, Wya, by, bçš„å­—å…¸
        char_to_ix -- å­—ç¬¦æ˜ å°„åˆ°ç´¢å¼•çš„å­—å…¸
        seed -- éšæœºç§å­
    è¿”å›ï¼š
        indices -- åŒ…å«é‡‡æ ·å­—ç¬¦ç´¢å¼•çš„é•¿åº¦ä¸ºnçš„åˆ—è¡¨ã€‚
    """

    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']
    vocab_size = by.shape[0]
    n_a = Waa.shape[1]

    x = np.zeros((vocab_size, 1))
    a_prev = np.zeros((n_a, 1))
    indices = []

    idx = -1

    # å¾ªç¯éå†æ—¶é—´æ­¥éª¤tã€‚åœ¨æ¯ä¸ªæ—¶é—´æ­¥ä¸­ï¼Œä»æ¦‚ç‡åˆ†å¸ƒä¸­æŠ½å–ä¸€ä¸ªå­—ç¬¦ï¼Œå¹¶å°†å…¶ç´¢å¼•é™„åŠ åˆ°â€œindicesâ€
    counter = 0
    newline_character = char_to_ix["\n"]

    while (idx != newline_character and counter < 50):
        a = np.tanh(np.dot(Wax, x) + np.dot(Waa, a_prev) + b)
        z = np.dot(Wya, a) + by
        y = cllm_utils.softmax(z)

        np.random.seed(counter + seed)

        # æ­¥éª¤3ï¼šä»æ¦‚ç‡åˆ†å¸ƒyä¸­æŠ½å–è¯æ±‡è¡¨ä¸­å­—ç¬¦çš„ç´¢å¼•
        idx = np.random.choice(list(range(vocab_size)), p=y.ravel())

        # æ·»åŠ åˆ°ç´¢å¼•ä¸­
        indices.append(idx)

        # æ­¥éª¤4:å°†è¾“å…¥å­—ç¬¦é‡å†™ä¸ºä¸é‡‡æ ·ç´¢å¼•å¯¹åº”çš„å­—ç¬¦ã€‚
        x = np.zeros((vocab_size, 1))
        x[idx] = 1
        a_prev = a

        seed += 1
        counter += 1

    if (counter == 50):
        indices.append(char_to_ix["\n"])

    return indices


def optimize(X, Y, a_prev, params, learning_rate=0.01):
    '''
    æ¨¡å‹å•æ­¥ä¼˜åŒ–
    '''
    loss, cache = cllm_utils.rnn_forward(X, Y, a_prev, params)
    grads, a = cllm_utils.rnn_backward(X, Y, params, cache)

    grads = clip(grads, 5)
    params = cllm_utils.update_parameters(params, grads, learning_rate)
    return loss, grads, a[len(X)-1]


def model(data, ix_to_char, char_to_ix, num_iterations=3500, n_a=50, dino_names=7, vocab_size=27):
    n_x, n_y = vocab_size, vocab_size
    parameters = cllm_utils.initialize_parameters(n_a, n_x, n_y)
    loss = cllm_utils.get_initial_loss(vocab_size, dino_names)

    with open("./data/dinos.txt") as f:
        examples = f.readlines()
    examples = [x.lower().strip() for x in examples]

    # æ‰“ä¹±å…¨éƒ¨çš„æé¾™åç§°
    np.random.seed(0)
    np.random.shuffle(examples)

    a_prev = np.zeros((n_a, 1))
    for j in range(num_iterations):
        index = j % len(examples)
        X = [None] + [char_to_ix[ch] for ch in examples[index]]
        Y = X[1:] + [char_to_ix['\n']]

        cur_loss, gradients, a_prev = optimize(X, Y, a_prev, parameters)

        loss = cllm_utils.smooth(loss, cur_loss)  # å¹³æ»‘æŸå¤±æ¥åŠ é€Ÿè®­ç»ƒ | æ¢¯åº¦å˜åŒ–ç›¸å¯¹æ›´å°ï¼Œè®­ç»ƒæ—¶ä¸å®¹æ˜“è·‘é£

        # æ¯2000æ¬¡è¿­ä»£ï¼Œé€šè¿‡sample()ç”Ÿæˆâ€œ\nâ€å­—ç¬¦ï¼Œæ£€æŸ¥æ¨¡å‹æ˜¯å¦å­¦ä¹ æ­£ç¡®
        if j % 200 == 0:
            print("ç¬¬" + str(j + 1) + "æ¬¡è¿­ä»£ï¼ŒæŸå¤±å€¼ä¸ºï¼š" + str(loss))

            seed = 0
            for name in range(dino_names):
                # é‡‡æ ·
                sampled_indices = sample(parameters, char_to_ix, seed)
                # cllm_utils.print_sample(sampled_indices, ix_to_char)

                seed += 1
    return parameters


def get_data():
    data = open('./data/dinos.txt', 'r').read()
    data = data.lower()
    chars = list(set(data))
    data_size, vocab_size = len(data), len(chars)
    print("Load data done, {} chars and {} unique chars".format(data_size, vocab_size))
    return data, chars


data, chars = get_data()
char_to_ix = {ch:i for i, ch in enumerate(sorted(chars))}
ix_to_char = {i:ch for i, ch in enumerate(sorted(chars))}


start_time = time.perf_counter()
parameters = model(data, ix_to_char, char_to_ix, num_iterations=5000)
end_time = time.perf_counter()
minium = end_time - start_time

print("æ‰§è¡Œäº†ï¼š" + str(int(minium / 60)) + "åˆ†" + str(int(minium%60)) + "ç§’")
